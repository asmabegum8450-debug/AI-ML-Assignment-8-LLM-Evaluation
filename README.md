# Assignment 8 â€” Evaluating Fine-Tuned LLM Performance

**Name:** [YOUR NAME]  
**Course:** [COURSE NAME]  
**Model:** [Your Fine-Tuned Model Name]  
**Task:** [Sentiment / Topic Classification]

---

## ğŸ“Œ Overview
This repository contains the complete evaluation of a fineâ€‘tuned Large Language Model (LLM) for a supervised textâ€‘classification task.  
The evaluation includes metric computation, confusion matrix generation, and detailed error analysis.

---

## ğŸ“‚ Repository Structure
```
.
â”œâ”€â”€ evaluation_notebook.ipynb
â”œâ”€â”€ confusion_matrix_normalized.png
â””â”€â”€ README.md
```

---

## ğŸ“Š Evaluation Metrics
The following metrics are computed using the heldâ€‘out test set:

| Metric | Value |
|--------|--------|
| Accuracy | [FILL] |
| Precision (Macro) | [FILL] |
| Recall (Macro) | [FILL] |
| F1-Score (Macro) | [FILL] |

---

## â­ Why Macro-F1?
Macroâ€‘F1 is preferred over Accuracy because it treats each class equally, making it more reliable in imbalanced datasets.  
Accuracy can remain high even when minority classes are misclassified.  
Macroâ€‘F1 evaluates both precision and recall per class and averages them fairly.

---

## ğŸ“‰ Confusion Matrix
A normalized confusion matrix is generated to visualize classification patterns.  
Rows represent **true** labels; columns represent **predicted** labels.

The matrix is saved as:
```
confusion_matrix_normalized.png
```

---

## ğŸ” Error Analysis
The notebook identifies and displays:

- The **worstâ€‘performing class** (lowest F1)
- Two misclassified samples from that class
- A technical explanation for why the model failed on each example

---

## ğŸ¥ Video Demonstration
Your recorded video should include:
- Metric explanation (Accuracy, Precision, Recall, F1)
- Confusion matrix interpretation
- Worstâ€‘class analysis
- Demonstration of misclassified samples
- Justification for using Macroâ€‘F1

---

## âœ… Conclusion
This repository provides a complete evaluation pipeline for analyzing LLM classification performance.
