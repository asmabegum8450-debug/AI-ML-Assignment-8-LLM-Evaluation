{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Evaluation Notebook\nThis notebook computes metrics, plots the confusion matrix, and performs error analysis."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score,\n    f1_score, confusion_matrix, classification_report\n)\n\nplt.rcParams['figure.figsize'] = (6,6)\nplt.rcParams['font.size'] = 11\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["test_df = pd.read_csv(\"data/test.csv\")  # update path\n\ntexts = test_df[\"text\"].tolist()\nlabels = test_df[\"label\"].tolist()\n\nlabel2id = {\n    \"negative\": 0,\n    \"neutral\": 1,\n    \"positive\": 2\n}\n\nid2label = {v: k for k, v in label2id.items()}\ny_true = [label2id[l] for l in labels]\nnum_classes = len(label2id)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["model_path = \"YOUR_MODEL_PATH\"  # update this\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nmodel.eval()\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def predict_batch(texts, batch_size=16):\n    all_logits = []\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        enc = tokenizer(\n            batch, padding=True, truncation=True,\n            return_tensors=\"pt\", max_length=256\n        )\n        enc = {k: v.to(device) for k,v in enc.items()}\n        with torch.no_grad():\n            out = model(**enc).logits.cpu().numpy()\n        all_logits.append(out)\n\n    logits = np.concatenate(all_logits)\n    preds = np.argmax(logits, axis=1)\n    return preds, logits\n\ny_pred, logits = predict_batch(texts)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["accuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\nrecall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\nf1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n\npd.DataFrame({\n    \"Metric\": [\"Accuracy\", \"Precision (Macro)\", \"Recall (Macro)\", \"F1 (Macro)\"],\n    \"Value\": [accuracy, precision, recall, f1]\n})\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["cm = confusion_matrix(y_true, y_pred)\ncm_norm = cm / cm.sum(axis=1, keepdims=True)\n\nfig, ax = plt.subplots()\nax.imshow(cm_norm, cmap=\"Blues\")\n\nax.set_xticks(range(num_classes))\nax.set_yticks(range(num_classes))\nax.set_xticklabels(label2id.keys(), rotation=45)\nax.set_yticklabels(label2id.keys())\n\nfor i in range(num_classes):\n    for j in range(num_classes):\n        ax.text(j, i, f\"{cm_norm[i,j]*100:.1f}%\", ha=\"center\", va=\"center\")\n\nax.set_xlabel(\"Predicted\")\nax.set_ylabel(\"True\")\nax.set_title(\"Normalized Confusion Matrix\")\n\nplt.tight_layout()\nplt.savefig(\"confusion_matrix_normalized.png\", dpi=300)\nplt.show()\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["report = classification_report(\n    y_true, y_pred,\n    target_names=list(label2id.keys()),\n    output_dict=True,\n    zero_division=0\n)\n\nclass_f1 = {cls: report[cls][\"f1-score\"] for cls in label2id.keys()}\nworst_class = min(class_f1, key=class_f1.get)\nworst_id = label2id[worst_class]\n\nprint(\"Worst class:\", worst_class)\n\nmis_idx = [i for i,(t,p) in enumerate(zip(y_true,y_pred)) if t==worst_id and p!=worst_id]\n\nprint(\"Misclassified examples:\", len(mis_idx))\n\nfor i in mis_idx[:2]:\n    print(\"\\n===================================\")\n    print(\"Index:\", i)\n    print(\"True label:\", id2label[y_true[i]])\n    print(\"Pred label:\", id2label[y_pred[i]])\n    print(\"Text:\", texts[i])\n"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}